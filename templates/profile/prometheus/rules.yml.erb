# Managed by puppet (nebula/profile/prometheus/rules.yml.erb)
groups:
- name: hardware
  rules:
  - alert: PrometheusNotRunning
    annotations:
      summary: 'Prometheus server {{$labels.hostname}} isn''t collecting metrics.'
    expr: 'absent(up{job="prometheus"})'
    for: 5m
    labels:
      severity: page
  - alert: PuppetBehind
    annotations:
      summary: 'Node {{$labels.host | reReplaceAll "\\.umdl\\.umich\\.edu" ""}} hasn''t recently synced with puppet.'
      dashboard: 'https://puppetboard.kubernetes.lib.umich.edu/node/{{$labels.host}}'
    expr: 'puppet_report{environment="production",host!="ht-web-preview.umdl.umich.edu"} < (time() - (30 * 60))'
    for: 30m
    labels:
      severity: ticket
  - alert: PuppetResourcesFailing
    annotations:
      summary: 'Node {{$labels.host | reReplaceAll "\\.umdl\\.umich\\.edu" ""}} has failing puppet resources.'
      dashboard: 'https://puppetboard.kubernetes.lib.umich.edu/node/{{$labels.host}}'
    expr: >
      sum without(name)(
        puppet_report_events{environment="production", name="Failure"}
      ) + sum without(name)(
        puppet_report_resources{environment="production", name="Failed"}
      ) > 0
    for: 2h
    labels:
      severity: ticket
  - alert: PuppetZeroResources
    annotations:
      summary: 'Node {{$labels.host | reReplaceAll "\\.umdl\\.umich\\.edu" ""}} has zero puppet resources.'
      dashboard: 'https://puppetboard.kubernetes.lib.umich.edu/node/{{$labels.host}}'
    expr: 'puppet_report_resources{environment="production", name="Total"} == 0'
    for: 2h
    labels:
      severity: ticket
  - alert: HostOomKillDetected
    annotations:
      summary: 'Linux node {{$labels.hostname}} OOM-killed a process'
    expr: 'increase(node_vmstat_oom_kill[5m]) > 0'
    for: 0m
    labels:
      severity: ticket
  - alert: HostUnderMemoryPressure
    annotations:
      summary: 'Linux node {{$labels.hostname}} is under memory pressure'
    expr: 'rate(node_vmstat_pgmajfault[1m]) > 1000'
    for: 2m
    labels:
      severity: info
  - alert: LinuxInstanceDown
    annotations:
      summary: 'Linux node {{$labels.hostname}} isn''t responding to Prometheus.'
    expr: 'up{job="node",hostname!="ht-web-preview"} == 0'
    for: 30m
    labels:
      severity: page
  - alert: PrometheusExporterDown
    annotations:
      summary: '{{$labels.job}} exporter isn''t responding to Prometheus.'
    expr: 'up{job!="node"} == 0'
    for: 30m
    labels:
      severity: page
  - alert: WindowsInstanceDown
    annotations:
      summary: 'Windows node {{$labels.hostname}} isn''t responding to Prometheus.'
    expr: 'up{job="wmi"} == 0'
    for: 30m
    labels:
      severity: page
  - alert: DiskSlowlyFillingUp
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} will fill up in a few days.'
    expr: >
      predict_linear(
        node_filesystem_avail_bytes{mountpoint!="/aspace",
                                    device!~".*/aspace",
                                    mountpoint!="/var/lockss",
                                    mountpoint!="/htdataden",
                                    fstype=~"nfs|cifs"}[1d],
        4 * 60 * 60 * 24
      ) < 0
    for: 1d
    labels:
      severity: info
  - alert: DiskAboutToFillUp
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} is filling up fast.'
    expr: >
      predict_linear(
        node_filesystem_avail_bytes{mountpoint!="/aspace",
                                    device!~".*/aspace",
                                    mountpoint!="/var/lockss",
                                    mountpoint!="/htdataden",
                                    fstype!="afs",
                                    fstype!="tmpfs",
                                    device!="rootfs"}[10m],
        4 * 60 * 60
      ) < 0
    for: 60m
    labels:
      severity: info
  - alert: DiskPressure
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} is more than 95% full.'
    expr: >
      (
        (
          avg_over_time(
            node_filesystem_size_bytes{mountpoint!="/usr", mountpoint!="/aspace", fstype!="afs", fstype!="nfs", fstype!="tmpfs", fstype!="cifs", device!="rootfs"}[1m]
          ) - avg_over_time(
            node_filesystem_avail_bytes[1m]
          )
        ) / avg_over_time(
          node_filesystem_size_bytes[1m]
        )
      ) > 0.95
    for: 30m
    labels:
      severity: ticket
  - alert: DiskFull
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} is full.'
    expr: >
      (
        (
          avg_over_time(
            node_filesystem_size_bytes{mountpoint!="/usr", mountpoint!="/aspace", fstype!="afs", fstype!="nfs", fstype!="tmpfs", fstype!="cifs", device!="rootfs"}[1m]
          ) - avg_over_time(
            node_filesystem_avail_bytes[1m]
          )
        ) / avg_over_time(
          node_filesystem_size_bytes[1m]
        )
      ) > 0.99
    for: 5m
    labels:
      severity: page
  - alert: HTDataDenIsFull
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} is full.'
    expr: >
      (
        (
          avg_over_time(
            node_filesystem_size_bytes{mountpoint="/htdataden"}[1m]
          ) - avg_over_time(
            node_filesystem_avail_bytes[1m]
          )
        ) / avg_over_time(
          node_filesystem_size_bytes[1m]
        )
      ) > 0.99
    for: 30m
    labels:
      severity: ticket
  - alert: DiskRunningOutOfINodes
    annotations:
      summary: 'Filesystem {{$labels.hostname}}:{{$labels.mountpoint}} is running out of inodes.'
    expr: >
      node_filesystem_files_free{fstype!="cifs",
                                 fstype!="vfat",
                                 fstype!="fuse.lxcfs",
                                 fstype!="fuse.s3fs",
                                 fstype!="rpc_pipefs"} < 10000
    for: 30m
    labels:
      severity: page
  - alert: WindowsUpdateBehind
    annotations:
      summary: 'Node {{$labels.hostname}} has not updated in 40+ days.'
    expr: 'time() - avg_over_time(windows_update[10m]) > 40 * 24 * 3600'
    for: 1h
    labels:
      severity: ticket
  - alert: MoreThanOneDeviceClaimingOneIP
    annotations:
      summary: 'More than one device claiming {{$labels.instance}}.'
    expr: 'count by (instance) (up) > 1'
    for: 5m
    labels:
      severity: ticket

  - alert: WwwLibPressFcgiUnreachable
    annotations:
      summary: 'Cannot reach press on node {{$labels.hostname}}.'
    expr: 'time() - press_fcgi_check_last_success > 300'
    for: 30m
    labels:
      severity: ticket

  - alert: DeepBlueDataUnreachable
    annotations:
      summary: 'Cannot reach Deep Blue Data on node {{$labels.hostname}}.'
    expr: 'time() - deep_blue_data_status_last_success > 300'
    for: 15m
    labels:
      severity: ticket

  # The staff-lib unison job is exempt because it routinely takes days
  # sometimes. The htapps-cache one also takes forever as a matter of
  # routine.
  - alert: UnisonSyncFailed
    annotations:
      summary: 'Unison sync {{$labels.client}} failed on node {{$labels.hostname}}.'
    expr: 'time() - unison_last_success{client!="htapps-cache",client!="nas-web-staff-lib"} > 5 * 60'
    for: 30m
    labels:
      severity: ticket

  - alert: MysqlBackupFailed
    annotations:
      summary: 'Mysql Backup has Failed on node {{$labels.hostname}}.'
    expr: 'time() - mysql_backup_last_success > 7 * 24 * 60 * 60'
    for: 1d
    labels:
      severity: ticket

  # We like to keep at least 3T free at all times for deep blue data,
  # but their ingest process requires double the capacity of any files
  # being ingested, so it is common for them to claim a lot of space
  # only temporarily. Hence the lengthy 1d alert threshold.
  - alert: DeepBlueDataProdStoragePressure
    annotations:
      summary: 'deepbluedata-prod ({{$labels.hostname}}:{{$labels.mountpoint}}) has less than 3T of free space'
    expr: 'node_filesystem_avail_bytes{device="deepbluedata-prod.value.storage.umich.edu:/deepbluedata-prod"} < 3 * 1024 * 1024 * 1024 * 1024'
    for: 1d
    labels:
      severity: ticket

  - alert: DeepBlueDataProdFedoraDown
    annotations:
      summary: 'Fedora is unresponsive on node {{$labels.hostname}}.'
    expr: 'time() - dbd_fedora_check_last_success > 65'
    for: 15m
    labels:
      severity: page

  # This doesn't appear to grow fast, so an 100G threshold should give
  # plenty of time to address any problems.
  - alert: HTDevStoragePressure
    annotations:
      summary: 'htdev ({{$labels.hostname}}:{{$labels.mountpoint}}) has less than 100G of free space'
    expr: 'node_filesystem_avail_bytes{device=~"htdev.value.storage.umich.edu:/htdev.*"} < 100 * 1024 * 1024 * 1024'
    for: 1d
    labels:
      severity: ticket

  - alert: DarkBlueFillingUp
    expr: >
      node_filesystem_avail_bytes{device="<%= @rules_variables['darkblue_device'] %>"} < (1 * 1024 * 1024 * 1024 * 1024)
    for: 10m
    labels:
      severity: ticket
    annotations:
      summary: 'Dark Blue repository storage, {{$labels.device}} is running low on available space.'
      description: '{{$labels.device}} has had less than 1TB of available space for more than 10 minutes.'
  - alert: DarkBlueFull
    expr: >
      node_filesystem_avail_bytes{device="<%= @rules_variables['darkblue_device'] %>"} < (100 * 1024 * 1024 * 1024)
    for: 5m
    labels:
      severity: page
    annotations:
      summary: 'Dark Blue repository storage, {{$labels.device}} is out of available space.'
      description: '{{$labels.device}} has had less than 100GB of available space for more than 5 minutes.'
  - alert: MysqlDown
    expr: >
      mysql_up == 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: 'MySQL is DOWN on {{$labels.hostname}}'
      description: 'MySQL is DOWN'
  - alert: MysqlSlaveSqlDown
    expr: >
      mysql_slave_status_slave_sql_running  == 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: 'MySQL Slave SQL is Not Running on {{$labels.hostname}}'
      description: 'MySQL Slave SQL is not Running'
  - alert: MysqlMaxPreparedStatment
    expr: >
      mysql_global_status_prepared_stmt_count / mysql_global_variables_max_prepared_stmt_count > 0.75
    for: 30m
    labels:
      severity: ticket
    annotations:
      summary: 'MySQL Prepared Statement Count is approaching MAX on {{$labels.hostname}}'
      description: 'MySQL Prepared Statement Count is approaching MAX'

  # These are aggregate rules for federated collection across our full
  # system. By putting them here, we essentially precompile them.
  - record: datacenter_role:network_transmit_bytes:rate5m
    expr: >
      sum without(device, hostname, instance)(
        rate(node_network_transmit_bytes_total[5m])
      )
  - record: datacenter_role:network_receive_bytes:rate5m
    expr: >
      sum without(device, hostname, instance)(
        rate(node_network_receive_bytes_total[5m])
      )
  - record: datacenter_role:node_cpu_seconds:max_not_idle_mean30s
    expr: >
      max without(hostname, instance)(
        avg without(cpu, mode)(
          1 - rate(node_cpu_seconds_total{mode="idle"}[30s])
        )
      )
  - record: datacenter_role:node_cpu_seconds:max_not_idle_mean2m
    expr: >
      max without(hostname, instance)(
        avg without(cpu)(
          sum without(mode)(
            rate(node_cpu_seconds_total{mode!="idle"}[2m])
          )
        )
      )
  - record: backend_datacenter:haproxy_server_up:count_servers_per_backend
    expr: >
      count without(server)(
        max without(hostname, instance, priority)(
          haproxy_server_up
        )
      )

  - record: backend_datacenter:haproxy_server_up:sum_servers_up_per_backend
    expr: >
      sum without(server)(
        max without(hostname, instance, priority)(
          haproxy_server_up
        )
      )
